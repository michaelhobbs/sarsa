{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<style>\n",
    ".jp-Notebook {\n",
    "    margin: auto;\n",
    "    max-width: 1024px;\n",
    "}\n",
    ".jp-InputPrompt {\n",
    "    display: none;\n",
    "}\n",
    "</style>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SARSA\n",
    "\n",
    "SARSA is a reinforcement learning algorithm. The name describes the algorithm: __S__tate __A__ction __R__eward __S'__tate __A'__ction.\n",
    "\n",
    "In its simplest form:\n",
    "- given a State _s_\n",
    "- refer to the policy and select an Action _a_\n",
    "- Loop\n",
    "    - observe the Reward _r_ for the selected action\n",
    "    - observe the State _s'_ given the current state _s_ and selected action _a_\n",
    "    - refer to the policy and select the next Action _a'_\n",
    "    - update Q(s, a) based on the observed reward and the selected states and actions\n",
    "    - repeat with s <- s', a <- a'\n",
    "\n",
    "In this report we will:\n",
    "\n",
    "1. Describe the problem statement\n",
    "2. Discuss how to implement SARSA given the problem statement, and illustrate with code snippets\n",
    "3. Explore the results, and measure performance and the effects of changing various parameters of the algorithm\n",
    "4. Cover on-policy (SARSA) as well as off-policy (Q-Learning) algorithms\n",
    "\n",
    "Our implementation will use an eligibility trace $TD(\\lambda)$ to speed up learning, and our policy will be $\\epsilon$-greedy. We will approximate the state in the continuous space, and model the experiment with a neural network.\n",
    "\n",
    "Libraries used: __numpy__ and __matplotlib__.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem Statement\n",
    "\n",
    "\n",
    "### Geography of the experiment\n",
    "A point-like agent is moving in a continuous 2d state space.\n",
    "  \n",
    "The 2d space is of unit length.\n",
    "\n",
    "A 20 by 20 grid of place cells, distributed uniformly, will be used to approximate the state given the agent's coordinates.\n",
    "\n",
    "A trial starts with the agent at $s_0=[0.1,\\ 0.1]$.\n",
    "\n",
    "At each step, the agent moves a distance of $l=0.03$ in one of 8 directions.\n",
    "\n",
    "A goal area is defined as a circle of radius 0.1 around the centre with coordinates $[0.8,\\ 0.8]$ \n",
    "\n",
    "![](./images/geometry_of_experiment.svg '') ![](./images/firstMoves.svg '')\n",
    "\n",
    "\n",
    "\n",
    "### Policy\n",
    "The direction is selected according to an $\\epsilon$-greedy policy:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " P= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\epsilon & \\quad random\\ a' \\\\\n",
    "            1- \\epsilon & \\quad \\max_{a'}Q(s, a')\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "A random direction is taken with probability $\\epsilon$ and the direction with the highest learnt reward will be selected with probability $1 - \\epsilon$.\n",
    "\n",
    "\n",
    "\n",
    "### Rewards\n",
    "$R=-2$ : If the agent's coordinates $s=[s_x,\\ s_y]$ are outside the grid after taking an action _a_, return the agent to the grid.\n",
    "\n",
    "$R=10$: If the agent is within 0.1 units from $[0.8,\\ 0.8]$.\n",
    "\n",
    "$R=0$: Any other action gets $R=0$.\n",
    "\n",
    "\n",
    "### Termination\n",
    "A trial stops either when the agent reaches the goal or after $N_{MAX}=10000$ steps.\n",
    "\n",
    "\n",
    "### Function approximation\n",
    "Approximate the state. Each cell has an activation depending on the agent's current coordinates: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  h_j(s)=\\exp({-\\frac{(x_j-s_x)^2+(y_j-s_y)^2}{2\\sigma^2})}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Each cell is connected to each action by a weight: $w_{aj}$.\n",
    "\n",
    "The Q function can then be estimated as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  Q(s,a)=\\sum_j{w_{aj}h_j(s)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement the SARSA and Q-Learning algorithms, and run 50 trials over 10 agents.\n",
    "\n",
    "Start with $\\epsilon=0.5$. Try different values and decaying over trials.\n",
    "\n",
    "Plot learning curves, total reward, number of steps to reach goal.\n",
    "\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "More formal algorithm definitions for Sarsa and Q-Learning, with and without eligibility trace:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    ">---\n",
    ">__Algorithm 1:__ _SARSA on-policy temporal difference TD(0)_\n",
    "\n",
    ">---\n",
    "> Algorithm parameters: step size  $\\alpha \\in (0 , 1],$ $\\epsilon > 0$   \n",
    "Initialize  $Q  ( s, a ), \\  \\forall s \\in S^+ , a \\in A ( s ),$ arbitrarily\n",
    ">\n",
    "> Loop for each episode:  \n",
    "$\\quad$Initialize $S$   \n",
    "$\\quad$Choose $A$ from $S$ using some policy derived from $Q$ (eg $\\epsilon$-greedy)  \n",
    "$\\quad$Loop for each step of episode:      \n",
    "$\\qquad$Take action $A$, observe $R,$ $S'$   \n",
    "$\\qquad$Choose $A'$ from $S'$ using the same policy as used to select $A$ (eg $\\epsilon$-greedy)   \n",
    "$\\qquad Q(S,A) \\leftarrow Q(S, A) + \\alpha[R+\\gamma $ <span style=\"background-color:lightgrey; padding: 1px;\">$Q(S', A')$</span>$ - Q(S, A)]$   \n",
    "$\\qquad S \\leftarrow S'$  \n",
    "$\\qquad A \\leftarrow A'$    \n",
    "$\\quad$until $S$ is terminal\n",
    "\n",
    ">---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    ">---\n",
    ">__Algorithm 2:__ _Q-Learning off-policy TD(0)_\n",
    "\n",
    ">---\n",
    "> Algorithm parameters: step size  $\\alpha \\in (0 , 1],$ $\\epsilon > 0$   \n",
    "Initialize  $Q  ( s, a ), \\  \\forall s \\in S^+ , a \\in A ( s ),$ arbitrarily   \n",
    ">\n",
    "> Loop for each episode:  \n",
    "$\\quad$Initialize $S$   \n",
    "$\\quad$Loop  for  each  step  of  episode:    \n",
    "$\\qquad$Choose  $A$ from $S$ using some policy derived from $Q$ (eg $\\epsilon$-greedy)   \n",
    "$\\qquad$Take action $A$, observe $R,$ $S'$   \n",
    "$\\qquad Q(S,A) \\leftarrow Q(S, A) + \\alpha[R+\\gamma$ <span style=\"background-color:lightgrey; padding: 1px;\">$\\max_{A'}Q(S', A')$</span>$ - Q(S, A)]$   \n",
    "$\\qquad S \\leftarrow S'$    \n",
    "$\\quad$until $S$ is terminal\n",
    "\n",
    ">---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    ">---\n",
    ">__Algorithm 3:__ _SARSA on-policy temporal difference TD($\\lambda$); with eligibility trace_\n",
    "\n",
    ">---\n",
    "> Algorithm parameters: step size  $\\alpha \\in (0 , 1],$ $\\epsilon > 0$   \n",
    "> Initialize  $Q  ( s, a ), \\  \\forall s \\in S^+ , a \\in A ( s ),$ arbitrarily  \n",
    "> Initialize $E(s,a), \\  \\forall s \\in S^+ , a \\in A ( s ),$ as 0s\n",
    ">\n",
    "> Loop for each episode:  \n",
    "$\\quad$Initialize $S$   \n",
    "$\\quad$Choose $A$ from $S$ using some policy derived from $Q$ (eg $\\epsilon$-greedy) \n",
    "$\\quad$Loop for each step of episode:      \n",
    "$\\qquad$Take action $A$, observe $R,$ $S'$   \n",
    "$\\qquad$Choose $A'$ from $S'$ using the same policy as used to select $A$ (eg $\\epsilon$-greedy)   \n",
    "$\\qquad \\delta = R+\\gamma Q(S', A') - Q(S, A) $  \n",
    "$\\qquad E(S,A) \\leftarrow E(S, A) + 1$, (note: use function approx increment for continuous space, instead of 1)  \n",
    "$\\qquad \\forall s,a:$  \n",
    "$\\quad\\qquad Q(s,a) \\leftarrow Q(s,a) + \\alpha\\delta $ <span style=\"background-color:lightgrey; padding: 1px;\">$E(s,a)$</span>  \n",
    "$\\quad\\qquad E(s,a) \\leftarrow \\gamma\\lambda E(s,a)$  \n",
    "$\\qquad S \\leftarrow S'$  \n",
    "$\\qquad A \\leftarrow A'$    \n",
    "$\\quad$until $S$ is terminal\n",
    "\n",
    ">---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "\n",
    "Starting by writing code snippets in a jupyter notebook, then a collection of functions and global variables, the final implementation will be class-based. We will touch on debugging, testing and performance.\n",
    "In this report simplified code snippets are provided. To read the source code and view how the plots were made, refer to the git repository [https://github.com/michaelhobbs/sarsa].\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import math"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agent\n",
    "The state is a 2d array of coordinates. $s = [s_x, s_y]$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent will need to be able to select a random direction, we wll need to determine **s'** given **s** and **a**, including handling movements at the edge of the grid and detecting when **s'** is in the goal area."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numDirections = 8\n",
    "def selectRandomDirection():\n",
    "\trandomDirection = np.random.randint(0, numDirections)\n",
    "\treturn randomDirection\n",
    "\n",
    "l = 0.03\n",
    "diagonalDelta = math.sqrt(l**2 /2)\n",
    "positionDeltas = np.array([ \\\n",
    "    [0, -l],  # down \\\n",
    "    [diagonalDelta, -diagonalDelta], # down right \\\n",
    "    [l, 0], # right \\\n",
    "    [diagonalDelta, diagonalDelta], # up right \\\n",
    "    [0, l], # up \\\n",
    "    [-diagonalDelta, diagonalDelta], # up left \\\n",
    "    [-l, 0], # left \\\n",
    "    [-diagonalDelta, -diagonalDelta] ]) # down left\n",
    "\n",
    "def updatePosition(currentCoords, direction):\n",
    "    return currentCoords + positionDeltas[direction]\n",
    "\n",
    "minCoords = [0, 0]\n",
    "maxCoords = [1, 1]\n",
    "def detectCollision(coords):\n",
    "    return coords[0] < minCoords[0] \\\n",
    "        or coords[1] < minCoords[1] \\\n",
    "        or coords[0] > maxCoords[0] \\\n",
    "        or coords[1] > maxCoords[1]\n",
    "\n",
    "def detectGoal(coords):\n",
    "\treturn np.sqrt((coords[0]-0.8)**2 + (coords[1]-0.8)**2) <= 0.1\n",
    "\n",
    "def returnRatToGrid(coords):\n",
    "\t'''return closest point inside grid to arg coords'''\n",
    "\treturn np.minimum(np.maximum(minCoords, coords), maxCoords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have all the building blocks for a single time step using a random selection of actions.\n",
    "\n",
    "Let's run 100'000 steps with this completely random agent.\n",
    "\n",
    "We will track rewards and follow the rules when collisions are detected by keeping the agent inside the grid when an action would take it outside.\n",
    "\n",
    "We will reset to the starting coordinates when the agent reaches the goal area."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reward = 0\n",
    "startingPosition = [0.1, 0.1]\n",
    "currentPosition = startingPosition\n",
    "history = np.array([currentPosition])\n",
    "iteration = 0\n",
    "while iteration < 100000:\n",
    "    randomDirection = selectRandomDirection()\n",
    "    newPosition = updatePosition(currentPosition, randomDirection) # s'\n",
    "    hasCollision = detectCollision(newPosition) # by taking action a given state s -> s'\n",
    "    hasGoal = detectGoal(newPosition) # by taking action a given state s -> s'\n",
    "    currentPosition = newPosition\n",
    "    currentReward = 0\n",
    "    if hasCollision:\n",
    "        currentReward = -2\n",
    "    elif hasGoal:\n",
    "        currentReward += 10\n",
    "    reward += currentReward\n",
    "\n",
    "    history = np.append(history, [currentPosition], axis=0)\n",
    "\n",
    "    if hasCollision:\n",
    "        currentPosition = returnRatToGrid(currentPosition)\n",
    "    if hasGoal:\n",
    "        currentPosition = startingPosition\n",
    "\n",
    "    iteration += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "States visited:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ![](./images/history.png '') ![](./images/historyHistogram.png '')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our agent has visited most states with the exception of some in the top-righthand corner of the grid, behind the goal area. This makes sense as the goal area blocks exploration behind it by resetting the agent to the starting point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network\n",
    "\n",
    "For the neural network we will need to calculate the activation of every neuron. We could do this with for loops, but this would not be very efficient. By vectorizing our neural network (weights, eligibility trace, input layer activation) we can perform faster vector operations with numpy. I decided to flatten all data into 2 dimensional vectors of length 20*20. The first 20 elements correspond to the first 20 cells in the grid along the bottom, the next 20 elements in the vecotrs are the second row of cells, and so on. The weights and eligibility trace have a second dimension: the directions. Each cell in the grid has 8 possible actions, and each of these is modelled by a weight, which converts the place cell activity _h(s)_ to its contribution towards picking that direction.\n",
    "\n",
    "First we will implement _h(s)_, and plot it to check what it looks like. \n",
    "\n",
    "Then we will implement the output neuron activation _Q(s,a)_.\n",
    "\n",
    "Finally, we will implement the eligibility trace, its update and decay.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### State approximation\n",
    "\n",
    "$\\begin{equation}\n",
    "  h_j(s)=\\exp({-\\frac{(x_j-s_x)^2+(y_j-s_y)^2}{2\\sigma^2})}\n",
    "\\end{equation}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculatePlaceCellActivity(ratCoords, placeCellCoords, sigma = 0.05):\n",
    "\tdenom = 2 * sigma**2\n",
    "\tnum = -((placeCellCoords[:, 0]-ratCoords[0])**2 + (placeCellCoords[:, 1]-ratCoords[1])**2)\n",
    "\treturn np.exp(num / denom)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Place cell activity visualized when agent is in the centre of the grid: $[0.5,\\ 0.5]$\n",
    "\n",
    "![](./images/placeCellActivity.svg '')\n",
    "\n",
    "Place cell activity visualized when agent is in the starting position: $[0.1,\\ 0.1]$\n",
    "\n",
    "![](./images/placeCellActivityStart.svg '')\n",
    "\n",
    "\n",
    "The closest neighbours contribute to the decision. The neighbours of the closest neighbours already do not contribute significantly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Output neuron activity\n",
    "\n",
    "$Q(s,a)=\\sum_j{w_{aj}h_j(s)}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xAxisSteps = 20\n",
    "yAxisSteps = 20\n",
    "weights = 0.001 * np.array(np.random.rand(xAxisSteps * yAxisSteps, numDirections))\n",
    "# Q(s, a)\n",
    "def calculateOutputNeuronActivityForDirection(ratCoords, direction, weights):\n",
    "\treturn np.sum(np.multiply(weights[:, direction], calculatePlaceCellActivity(ratCoords, placeCellCoords)))\n",
    "\n",
    "def calculateOutputNeuronActivity(ratCoords, weights):\n",
    "\treturn [calculateOutputNeuronActivityForDirection(ratCoords, a, weights) for a in range(numDirections)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The eligibility trace is first increased by the activation of the input neurons for the selected action. This is the contribution of each place cell towards the decision:\n",
    "$$\n",
    "e_a(t) = e_a(t-1) + h(s)\n",
    "$$\n",
    "\n",
    "This state of the eligibility trace is used in the weight update. Weights are updated by:\n",
    "$$\\Delta_w = \\eta * e_a * (r - Q(s,a) + \\gamma * Q(s',a'))$$\n",
    "Where:\n",
    "$\\eta$: learning rate\n",
    "$\\gamma$: reward discount rate\n",
    "\n",
    "After updating the weights, the entitre eligibility trace will decay. This prevents actions taken a long time ago from affecting the learning as much as the actions which just occurred : \n",
    "$$\n",
    "e_a(t) = \\lambda * \\gamma * (e_a(t-1) + h(s))\n",
    "$$\n",
    "Where:\n",
    "$\\lambda$: eligibility trace decay rate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eligibilityTrace = np.zeros((xAxisSteps * yAxisSteps, numDirections)) # at start of experiment there is no trace\n",
    "\n",
    "def updateEligibilityTrace(currentCoords, direction, eligibilityTrace):\n",
    "\treturn eligibilityTrace[:, direction] + calculatePlaceCellActivity(currentCoords, placeCellCoords)\n",
    "\n",
    "def updateWeights(delta, weights, eligibilityTrace):\n",
    "\treturn weights + (learningRate * delta * eligibilityTrace)\n",
    "\n",
    "def decayEligibilityTrace(eligibilityTrace):\n",
    "\treturn eligibilityTrace * rewardDiscountRate * eligibilityTraceDecayRate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single Step\n",
    "\n",
    "Here we simply put everything together in a single function. We will adapt and extend thet code snippet we wrote to test the grid setup, where we ran a randomly acting agent for 100'000 time steps.\n",
    "\n",
    "The only part missing is the $\\epsilon$-greedy policy.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def selectDirection(epsilon, ratCoords, weights):\n",
    "\trandomNumber = np.random.rand(1)\n",
    "\tif (randomNumber < epsilon):\n",
    "\t\treturn selectRandomDirection()\n",
    "\telse:\n",
    "\t\toutputNeuronActivity = calculateOutputNeuronActivity(ratCoords, weights)\n",
    "\t\tarr = np.array(outputNeuronActivity)\n",
    "\t\tmaxElement = np.amax(arr)\n",
    "\t\tmaxIndexes = np.where(arr == maxElement)\n",
    "\t\trandomFromMaxIndexes = np.random.choice(maxIndexes[0]) # pick random index when several share maxValue\n",
    "\t\treturn randomFromMaxIndexes\n",
    "\n",
    "def runSingleTimeStep(currentPosition, currentDirection, weights, eligibilityTrace, epsilon):\n",
    "\treward = 0\n",
    "\tnewPosition = updatePosition(currentPosition, currentDirection) # s'\n",
    "\thasCollision = detectCollision(newPosition) # by taking action a given state s -> s'\n",
    "\thasGoal = detectGoal(newPosition) # by taking action a given state s -> s'\n",
    "\tif (hasCollision):\n",
    "\t\tnewPosition = returnRatToGrid(newPosition)\n",
    "\t\treward -= 2\n",
    "\telif (hasGoal):\n",
    "\t\treward += 10\n",
    "\t# update eligibility trace at ratCoords before move for chosen direction\n",
    "\teligibilityTrace = updateEligibilityTrace(currentPosition, currentDirection, eligibilityTrace)\n",
    "\n",
    "\t# calculate next action a' which will be taken in next iteration and will be used to update the model\n",
    "\tnewDirection = selectDirection(epsilon, newPosition, weights)\n",
    "\tanticipatedOutputNeuronActivityInNextStep = calculateOutputNeuronActivityForDirection(newPosition, newDirection, weights)\n",
    "\n",
    "\tdelta = reward - calculateOutputNeuronActivityForDirection(currentPosition, currentDirection, weights) \\\n",
    "         + rewardDiscountRate * anticipatedOutputNeuronActivityInNextStep\n",
    "\tweights = updateWeights(delta, weights, eligibilityTrace)\n",
    "\teligibilityTrace = decayEligibilityTrace(eligibilityTrace)\n",
    "\treturn [newPosition, newDirection, reward, hasGoal, weights, eligibilityTrace]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trial\n",
    "\n",
    "A trial takes an agentand runs time steps until reaching an end condition. As a reminder, end conditions are running N_max steps without the agent finding the goal area, or the agent entering a state within the goal area.\n",
    "\n",
    "We will wrap the single time step function in a loop until a trial termination condition is met. You will notice some print and file io operations, this is to be able to see more or less how far along the execution of the episode is while it is running, and saves data for analysis later. It is useful to save as much data as possible because the execution may take a lot of time, and it is very inefficient to have to re-run the entire experiment to collect more data later.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def runTrial(weights, eligibilityTrace, epsilon, trialNumber, experimentNumber):\n",
    "\tratCoords = [0.1, 0.1]\n",
    "\tcurrentDirection = selectDirection(epsilon, ratCoords, weights) # initial action a\n",
    "\ttrialReward = 0\n",
    "\tmaxSteps = 10000\n",
    "\tcurrentStep = 0\n",
    "\thasGoal = False\n",
    "\n",
    "\twhile (currentStep < maxSteps and not hasGoal):\n",
    "\t\t[ratCoords, currentDirection, reward, hasGoal, weights, eligibilityTrace] = \\\n",
    "\t\t\t runSingleTimeStep(ratCoords, currentDirection, weights, eligibilityTrace, epsilon)\n",
    "\t\t# track coords for analysis\n",
    "\t\ttrialReward += reward\n",
    "\t\tcurrentStep += 1\n",
    "\n",
    "\t# save all coords for analysis\n",
    "\treturn [weights, eligibilityTrace, hasGoal, currentStep, trialReward, ratCoords]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment\n",
    "\n",
    "Our experiment will consist of 50 trials per agent.\n",
    "\n",
    "We will wrap the `runTrial` function in an `runExperiment` function. It will be useful to save the weights (Q-values) between trials to see how they evolve. Tracking trial reward is also interesting to measure progress. Print and file operations have been omitted in the report but can be found in the source code in github."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def runExperiment(experimentNumber):\n",
    "    maxTrials = 50\n",
    "    currentTrial = 0\n",
    "    epsilon = 0.9\n",
    "    weights = 0.001 * \\\n",
    "        np.array(np.random.rand(xAxisSteps * yAxisSteps, numDirections))\n",
    "    while (currentTrial < maxTrials):\n",
    "        eligibilityTrace = np.zeros((xAxisSteps * yAxisSteps, numDirections)) # reset eligibility trace\n",
    "        [weights, eligibilityTrace, hasGoal, currentStep, trialReward, ratCoords] = runTrial(\n",
    "            weights, eligibilityTrace, epsilon, currentTrial, experimentNumber)\n",
    "        # optionally update epsilon\n",
    "        # save weights for analysis\n",
    "        currentTrial += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Debugging and Tests\n",
    "\n",
    "In order to write tests your functions have to be importable. The easiest way to do this is to separate your main script (the one you run) and your functions. Then you can import your functions in your test file and write unit tests.\n",
    "\n",
    "Refer to the source code in github to see some tests for detecting collisions and goal states, as well as place cell activity and Q value calculation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "### Theoretical best\n",
    "Euclidean distance between starting point and centre of goal area, minus the radius of the goal area.\n",
    "\n",
    "$\\sqrt{(0.8-0.1)^2+(0.8-0.1)^2}-0.1 = 0.8899$\n",
    "\n",
    "Distance to closest point of goal area by movement speed:\n",
    "\n",
    "$\\frac{0.8899}{0.03} = 29.66$\n",
    "\n",
    "The shortest path to the goal consists of 30 steps.\n",
    "\n",
    "### On- vs. Off- Policy\n",
    "Sarsa is very similar to Q-Learning. The only difference between the two algorithms is that while Sarsa uses the action selected for the weight update as the next action the agent will take, Q-Learning updates the weights using the best action available at the next time, given its current understanding of the \\[state, action\\] space. You will find the update equations below:\n",
    "\n",
    "__Sarsa__\n",
    "$\\Delta_w = \\eta * e_a * (r - Q(s,a) + \\gamma * Q(s',a'))$\n",
    "\n",
    "__QL__\n",
    "$\n",
    "\\begin{equation}\n",
    "\\Delta_w = \\eta * e_a * (r - Q(s,a) + \\gamma * Q_{max_{a'}}(s',a'))\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "In general Q-Learning's approach promotes exploration, however, when using a funcation approximation for the state it is not guaranteed to converge. This is because the behaviour policy and the target policy are different (Baird 1995). i.e. the action used in the update rule is different to the action actually taken in the next step.\n",
    "\n",
    "In our example experiment, off-policy learning fails to converge. The weights (Q-values) explode and eventually become `NaN`s. More elaborate algorithms have been proposed to ensure convergence while still addressing the curse of dimensionality with continuous state and very large state sets.\n",
    "\n",
    "The take-away here is that not all learning algorithms can be applied to the continuous state space, and the algorithm to use needs to be selected depending on the problem statement.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploration vs. Exploitation\n",
    "\n",
    "In order to learn the approximate future rewards (Q-values) for each action in each state, the agent should visit and take these actions. Since the future reward depends on all future actions, the number of possible paths to learn and to take into account when estimating the future reward grows exponentially.\n",
    "\n",
    "The $\\epsilon$ parameter allows us to control how often the agent takes a random direction. In this way, the agent can explore the state space, and better estimate the future rewards. This parameter also allows the agent to break out of sub-optimal paths, by sometimes moving randomly the agent may find shorter paths.\n",
    "\n",
    "In this report we compare various strategies for managing the value of $\\epsilon$:\n",
    "- **constant value**. We tried values in \\[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1\\]\n",
    "- **linear decay** from 1 to 0. Decreases the same amount at each new trial until reaching the minimum of 0.\n",
    "- **logistic decay** following: $log(10-(10-1)*(t/49))$\n",
    "- **exponential decay** following: $exp(-t/exp(1))$\n",
    "\n",
    "Visually, it is easier to understand how these strategies will affect learning.\n",
    "\n",
    "![](./images/linDecay.svg '')![](./images/logDecay.svg '')![](./images/expDecay.svg '')\n",
    "\n",
    "**Exponential decay** will have a short exploration phase.\n",
    "\n",
    "**Linear decay** will give equal weight to exploration and exploitation. Starting by heavily exploring the actions, it will gradually enter an exploitation phase.\n",
    "\n",
    "**Logistic decay** will prioritize exploration, and only start eploitation in the final couple of trials.\n",
    "\n",
    "#### Performance\n",
    "Plotted below are the mean rewards observed in each of the trial runs of the experiment. This has been averaged out over 10 agents. Each line is using a different value/strategy of $\\epsilon$.\n",
    "\n",
    "![](./images/avgReward.svg '')\n",
    "![](./images/averageNumSteps.svg '')\n",
    "\n",
    "From this first glance into the results, we observe that the completely random agent ($\\epsilon$=0). The random agent has a high trial duration and very low reward. The opposite agent ($\\epsilon$=1), also has a very high trial duration at the end of the experiment, however the agent learnt to avoid the negative rewards and does not have a low average reward. This agent sometimes gets stuck in sub-optimal loops and never explores outside them.\n",
    "\n",
    "Zooming into the last 10 trials and only plotting the strategies which performed well, we get the following plots:\n",
    "\n",
    "![](./images/exploitingReward.svg '')\n",
    "![](./images/exploitingDuration.svg '')\n",
    "\n",
    "Two well performing $\\epsilon$ strategies failed to cinsistently achieve the maximum reward: constant $\\epsilon$ values 0.75 and 0.5. In addition to not achieving top rewards, their trial duration was higher than the best scoring strategies and were not included in the zoom-in plot of the trial durations.\n",
    "\n",
    "The average duration plot shows that the exponential decay strategy suffered from having a shorter exploration period. The constant value strategies were beaten by the linear and logistic decay strategies, with the agent taking a random direction 10% of the time performing better than the 25% random agent.\n",
    "\n",
    "Finally, the linear agent performed best until the very end, when the logistic agent entered the fully exploitation phase. The logistic decay strategy resulted in the shortest path (30 steps) being discovered by most of the 10 agents who ran the experiment.\n",
    "\n",
    "Table of final performance measured as average reward and duration of trials taken over the final 5 trials.\n",
    "\n",
    "| Strategy      | Reward           | Duration  |\n",
    "| :------------ |:----------------:| ---------:|\n",
    "| 0             | 6.9              | 4024.6    |\n",
    "| 0.1           | 9.2              | 45.0      |\n",
    "| 0.25          | 7.8              | 56.1      |\n",
    "| 0.5           | 7.2              | 96.7      |\n",
    "| 0.75          | 7.6              | 152.8     |\n",
    "| 0.9           | -1.4             | 453.2     |\n",
    "| 1             | -240.1           | 2573      |\n",
    "| linear        | -3.7             | 36.2      |\n",
    "| logistic      | -7.3             | 35.7      |\n",
    "| exponential   | 4.76             | 53.8      |\n",
    "\n",
    "\n",
    "Table of final performance measured as average reward and duration of trials taken over the final trial.\n",
    "\n",
    "\n",
    "| Strategy      | Reward           | Duration  |\n",
    "| :------------ |:----------------:| ---------:|\n",
    "| 0             | 6                | 4024.6    |\n",
    "| 0.1           | 10               | 44.4      |\n",
    "| 0.25          | 10               | 57.4      |\n",
    "| 0.5           | 10               | 90.2      |\n",
    "| 0.75          | 8.6              | 156.1     |\n",
    "| 0.9           | -2.8             | 533.7     |\n",
    "| 1             | -245.8           | 2519.8    |\n",
    "| linear        | 10               | 34.7      |\n",
    "| logistic      | 10               | 30.8      |\n",
    "| exponential   | 10               | 53.2      |\n",
    "\n",
    "\n",
    "These results demonstrate the need for a balance between exploration (high $\\epsilon$) and exploitation (low $\\epsilon$). One can imagine a good approach being to initially set a high value to $\\epsilon$, and as the agent discovers and learns the (state, action)-space, decrease it. One could also imagine approaches where this paremeter would be hand-tuned over a period of days, or maybe modifying automatically following some rules on the observed reward. eg: if a trial results in a low reward, increase $\\epsilon$ and enter an exploration phase; and if a trial results in a high reward, descrease $\\epsilon$ (enter exploitation phase)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Elegibility Trace\n",
    "\n",
    "Learning with an eligibility trace means that Q-value updates are applied to the previous \\[state, action\\] pairs visited. Without an eligibility trace, the only Q-value updated is the \\[state, action\\] pair which triggered the reward. As a result, the agent must run paths multiple times in order for reward information to trickle back to the earlier states in the path to a reward state.\n",
    "\n",
    "As we are using function approximation, our Q-value updates will already affect a neighbourhood of states, even without the use of an eligibility trace. Since our problem is very simple and when a reward is observed for an action, it is valid for almost all states using that same action, the eligibility trace speed up is not very prominent. In fact we only observe the speed up in the first few trials.\n",
    "\n",
    "![](./images/durationWithElig.svg '')\n",
    "![](./images/durationWithoutElig.svg '')\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evolution of weights\n",
    "It can help to visualise the evolution of the Q-values, our weights. Here we will demonstrate one way these can be visualised.\n",
    "\n",
    "**Quiver plot**\n",
    "For each place cell we will find which action has the largest Q-value, and we will plot an arrow centred at the place cell coordinates in the direction corresponding to the largest Q-value. We illustrate with quiver plots generated before and after training. $\\epsilon = 0.5$\n",
    "\n",
    "![](./images/quiver/multi.svg '')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weight initialization\n",
    "To gain insight in how important weight initialization is for Sarsa, the experiment has been run with all other parameters fixed, $\\epsilon = 0.5$, using three weight initialization strategies, as always averaged over 10 agents:\n",
    "- Small random values in \\[0, 0.001\\]\n",
    "- Zeros\n",
    "- Random in \\[0, 1\\]\n",
    "\n",
    "![](./images/weightInitRewards.svg '') ![](./images/weightInitSteps.svg '')\n",
    "\n",
    "One agent failed to find a path to the goal for the first 40 trials when weights were initialized using very small random values. This heavily penalises that strategy when comparing to the others. The zero weight initialization seems to have performed best. All the same, there is no strong winner, and the low values initialization may have simply been unlucky. Were we to run using 100 agents instead of only 10, perhaps we would see the other strategies also having agents which fail to find the goal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusions\n",
    "\n",
    "We saw the trade off between exploration and exploitation in reinforcement learning. An agent with no prior knowledge of the environment needs to explore it before being able to exploit its understanding of the environment. In Sarsa, this can be controlled by the value of $\\epsilon$ when using an $\\epsilon$-greedy policy.\n",
    "\n",
    "We saw how to use function approximation to model continuous state spaces, and discovered that Q-Learning (off-policy) will not always converge when using function approximation.\n",
    "\n",
    "We showed how an eligibility trace can be used to speed up propagation of observed rewards, speeding up learning, and saw that measuring performance using only 10 agents can suffer from \"noise\". eg: if one of the 10 agents performs badly, due to randomness in the action selection, then the average performance measured suffers greatly. We proposed a way to mitigate this: running over more agents averaging out a performance measurement.\n",
    "\n",
    "We considered 3 weight initialization strategies and found that initializing weights to 0s seems to be a valid choice, at least in our environment.\n",
    "\n",
    "An implementation was proposed, using vectorization for array operations and we generated human-readable representations of the learning process.\n",
    "\n",
    "## Open questions / Future tasks / Possible extensions\n",
    "\n",
    "How should we manage $\\epsilon$ over time were the grid layout to change? e.g. the goal area moves every 20 episodes.\n",
    "\n",
    "How would we tackle the problem of a \"shadowed\" optimal goal? We saw that our agent barely ever reaches behind the goal area. If a second goal area with a much higher reward were put in the top righthand corner of the grid, how should we set our parameters to increase the likelihood of discovery of the path to the highest paying goal area?\n",
    "\n",
    "Could a deep neural network improve performance?\n",
    "Fitted Q-Iteration (FQI); Experience Replay in Deep Q-networks (DQN); Least-square temporal difference (LSTD)\n",
    "\n",
    "Could we add a second agent, a hunter, whose target is to catch the first agent? (positive reward when in the same cell as the first agent). How would we have to tweak the parameters? How many states would we have and how would we adapt our function approximation?\n",
    "\n",
    "Can we improve performance and speed up our implementation? Profile the code to find the slowest parts of the implementation. (hint: try caching Q-values and function approx)\n",
    "\n",
    "Can we build a maze and test our learning algorithm on a more complicated environment? How long would agents need to learn such an environment?  \n",
    "_See `maze.py` and `maze_` prefixed files under `images` for an example of how this may be implemented and to get an idea of how this performs (esp. images/animations/mega_maze/maze_50_17.png)_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bonus:\n",
    "\n",
    "Videos of final trial of learning process for different epsilon strategies.\n",
    "\n",
    "<video controls=\"controls\" poster=\"./images/animations/last_trial_0.1.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/last_trial_0.1.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "<video controls=\"controls\" poster=\"./images/animations/last_trial_0.5.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/last_trial_0.5.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "<video controls=\"controls\" poster=\"./images/animations/last_trial_0.9.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/last_trial_0.9.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "<video controls=\"controls\" poster=\"./images/animations/last_trial_expDecay.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/last_trial_expDecay.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "<video controls=\"controls\" poster=\"./images/animations/last_trial_linearDecay1-0.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/last_trial_linearDecay1-0.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "<video controls=\"controls\" poster=\"./images/animations/last_trial_logDecay.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/last_trial_logDecay.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "\n",
    "\n",
    "### Completely random agent when epsilon set to 0\n",
    "\n",
    "The agent trained with epsilon = 1 spent the entire experiment in exploration mode and I expected it to perform best. However, the Quiver plot of argmax(Q) and running an extra trial using the Q-values of a random agent, but this time setting it to only exploit ($\\epsilon=0$), show that this agent failed to find the shortest path to the goal. It managed to have all edge states pointing into the grid, but the rest looks almost random. I suppose this is due to the eligibility trace always being full of random actions when a goal state is reached.\n",
    "\n",
    "![](./images/exploit/quiver.svg '')\n",
    "\n",
    "<video controls=\"controls\" poster=\"./images/animations/explorer.png\">\n",
    "  <source type=\"video/mp4\" src=\"./images/animations/explorer.mp4\"></source>\n",
    "  <p>Your browser does not support the video element.</p>\n",
    "</video>\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd08ddb8570d3e1ee7f4a1f490a8bc504230a9ab2f2e54adbd67e24fb0ff9aff77b",
   "display_name": "Python 3.9.1 64-bit ('sarsa': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8ddb8570d3e1ee7f4a1f490a8bc504230a9ab2f2e54adbd67e24fb0ff9aff77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}